from langchain_core.messages import HumanMessage,AIMessage
from typing import Literal
from langgraph.graph import StateGraph, START, END
from langchain_ollama import ChatOllama
from langgraph.prebuilt import create_react_agent
from agent_retriever import get_retriever
from langgraph.graph import MessagesState
from typing_extensions import TypedDict

# from agent_supervisor import supervisor_node

members = ['retriever', 'assistant']
gemma2_2btool = "cow/gemma2_tools:2b"
llama3_2 = "llama3.2:3b"
llama3_1 = "llama3.1"

llm_retriever = ChatOllama(model=llama3_2, temperature=0.2, streaming=True)
llm = ChatOllama(model=gemma2_2btool, temperature=0.5, streaming=True)
llm_sup = ChatOllama(model=llama3_1, temperature=0.3, streaming=True)
options = members + ["FINISH"]


class AgentState(MessagesState):
    next: str


retriever_agent = create_react_agent(
    llm_retriever, tools=[get_retriever],
    state_modifier="Báº¡n lÃ  má»™t ngÆ°á»i truy xuáº¥t sáº£n pháº©m sau Ä‘Ã³ tá»•ng há»£p thÃ´ng tin láº¡i vÃ  tráº£ vá». KhÃ´ng lÃ m gÃ¬ khÃ¡c."
)

assistant_prompt = """
    Báº¡n lÃ  Assistant AI, NhÃ¢n viÃªn tÆ° váº¥n khÃ¡ch hÃ ng cá»§a HD Bank.
    HÃ£y tráº£ lá»i ngáº¯n dá»n dá»±a trÃªn cÃ¡c thÃ´ng tin Ä‘Æ°á»£c cung cáº¥p.
    á»ž cuá»‘i cÃ¢u tráº£ lá»i luÃ´n há»i "Báº¡n cÃ³ tháº¯c máº¯c Ä‘iá»u gÃ¬ khÃ´ng ? ðŸ¤—"
"""

assistant_agent = create_react_agent(
    llm, tools=[], state_modifier=assistant_prompt
)


def assistant_node(state: AgentState) -> AgentState:
    result = assistant_agent.invoke(state)
    if result.get("messages"):
        response_content = result["messages"][-1].content
    else:
        response_content = "KhÃ´ng cÃ³ káº¿t quáº£."
    return {
        "messages": [
            AIMessage(content=response_content, name="assistant")
        ]
    }


def retriever_node(state: AgentState) -> AgentState:
    result = retriever_agent.invoke(state)
    if result.get("messages"):
        response_content = result["messages"][-1].content
    else:
        response_content = "KhÃ´ng cÃ³ káº¿t quáº£."
    return {
        "messages": [
            AIMessage(content=response_content, name="retriever")
        ]
    }


class Router(TypedDict):
    next: Literal[*options]


system_prompt = (
    "Báº¡n lÃ  ngÆ°á»i giÃ¡m sÃ¡t tÆ° váº¥n khÃ¡ch hÃ ng cá»§a ngÃ¢n hÃ ng HD Bank, "
    "Báº¡n giÃ¡m sÃ¡t vÃ  quáº£n lÃ½ cuá»™c trÃ² chuyá»‡n giá»¯a nhá»¯ng ngÆ°á»i sau : "
    f"{members}, "
    "XÃ¡c Ä‘á»‹nh ná»™i dung yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. Tráº£ vá» ai lÃ  ngÆ°á»i thá»±c hiá»‡n hÃ nh Ä‘á»™ng tiáº¿p theo, "
    "Má»—i member sáº½ thá»±c hiá»‡n vÃ  tráº£ vá» káº¿t quáº£ vÃ  tráº¡ng thÃ¡i. "
    "Náº¿u asisstant Ä‘Ã£ tráº£ lá»i Ä‘Æ°á»£c ngÆ°á»i dÃ¹ng, hÃ£y tráº£ vá» FINISH."
)
# system_prompt = (
#     "You are a supervisor tasked with managing a conversation between the"
#     f" following workers: {members}. Given the following user request,"
#     " respond with the worker to act next. Each worker will perform a"
#     " task and respond with their results and status. When finished,"
#     " respond with FINISH."
# )

def supervisor_node(state: AgentState) -> AgentState:
    messages = [
                   {"role": "system", "content": system_prompt},
               ] + state["messages"]

    response = llm_sup.with_structured_output(Router).invoke(messages)
    next_ = response["next"]

    # Log giÃ¡ trá»‹ quyáº¿t Ä‘á»‹nh tiáº¿p theo
    print(f"Supervisor decision: {next_}")

    if next_ == "FINISH":
        next_ = END
        print("Finishing conversation...")  # Log khi káº¿t thÃºc

    return {"next": next_}


builder = StateGraph(MessagesState)
builder.add_edge(START, "supervisor")
builder.add_node("supervisor", supervisor_node)
builder.add_node("retriever", retriever_node)
builder.add_node("assistant", assistant_node)

for member in members:
    builder.add_edge(member, "supervisor")

builder.add_conditional_edges("supervisor", lambda state: state["next"])
builder.add_edge(START, "supervisor")

graph = builder.compile()

# Hiá»ƒn thá»‹ vÃ  lÆ°u hÃ¬nh áº£nh Ä‘á»“ thá»‹
from IPython.display import display, Image

display(Image(graph.get_graph().draw_mermaid_png()))
with open("graph.png", "wb") as f:
    f.write(graph.get_graph().draw_mermaid_png())

# Cháº¡y Ä‘á»“ thá»‹ vá»›i input
for s in graph.stream(
        {
            "messages": [
                (
                        "user",
                        "Xin chÃ o",
                )
            ]
        },
        subgraphs=True,
):
    print(s)
    print("----")
